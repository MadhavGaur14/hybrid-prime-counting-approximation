\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{array}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{placeins}


\geometry{margin=1in}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}

\title{A Hybrid Rational Approximation for Prime Counting: \\
Rigorous Construction and Analysis of $\pi_h(x)$\thanks{Code and data available at: \texttt{https://github.com/MadhavGaur14/hybrid-prime-counting-approximation} and archived at \texttt{https://doi.org/10.5281/zenodo.18749890}}}
\author{Madhav Gaur}
\date{February 25, 2026}

\begin{document}

\maketitle

\begin{abstract}
We present an experimental investigation of $\pi_h^{(N)}(x)$, a parametric family of hybrid rational approximations to the prime-counting function $\pi(x)$. The construction $\pi_h^{(N)}(x) = \sum_{n=1}^{N} \frac{\pi_g(x)+x^{1/n}}{(\ln x)^n}$ combines a base rational approximant $\pi_g(x) = \frac{x(\ln x+1)}{(\ln x)^2+1}$ with geometrically damped fractional-power correction terms. We prove explicit truncation error bounds $E_N(x) \leq \frac{C_N x}{(\ln x)^{N+2}}$ with computable constants $C_N$ (e.g., $C_3 = 2.80$ for $\ln x \geq 100$) derived from elementary inequalities via monotone domination arguments. \\ \\
Systematic empirical validation at 20 test points ($x = 10^n$, $n = 1, \ldots, 20$) demonstrates that $\pi_h^{(3)}(x)$ achieves intermediate accuracy between the classical $x/\ln x$ approximation and the logarithmic integral Li$(x)$, with absolute error reductions by factors of 2--3.5 relative to $x/\ln x$ when compared against known values of $\pi(x)$. For $N = 3$, the approximation requires $O(N)$ arithmetic operations per evaluation. \\ \\
This work contributes to the experimental literature on approximation design: we demonstrate that explicitly analyzable hybrid forms can achieve measurable accuracy improvements with predictable, rigorously bounded truncation error, providing a case study in the construction and validation of elementary prime-counting approximations.
\end{abstract}

\section{Introduction}

The prime-counting function $\pi(x)$, which enumerates primes not exceeding $x$, is a central object in analytic number theory. The Prime Number Theorem, with its asymptotic form $\pi(x) \sim x/\ln x$, and refined approximations like the logarithmic integral $\text{Li}(x) = \int_2^x \frac{dt}{\ln t}$, provide the theoretical foundation for understanding prime distribution.

\subsection*{Motivation and Scope}

This paper presents an experimental investigation of $\pi_h^{(N)}(x)$, a parametric family of hybrid rational approximations to the prime-counting function $\pi(x)$. Our contribution is methodological: we construct an approximation with explicit, computable truncation error bounds—$E_N(x) \leq C_N \cdot x/(\ln x)^{N+2}$ with constants derived rigorously from elementary inequalities—and systematically validate its empirical performance across nine orders of magnitude. The construction combines a stable rational base term with geometrically damped fractional-power corrections, yielding an approximation whose computational cost is fixed (for chosen truncation level $N$) while providing accuracy intermediate between classical $x/\ln x$ and the logarithmic integral $\text{Li}(x)$.

We position this work as an experimental study: the goal is not to advance asymptotic theory or algorithmic complexity, but to demonstrate that simple, explicitly analyzable forms can achieve measurable accuracy improvements with predictable error control, providing a case study in the design space of elementary prime-counting approximations.

\subsection*{Construction and Contributions}

The hybrid approximation is defined as
\[
\pi_h^{(N)}(x) = \sum_{n=1}^{N} \frac{\pi_g(x) + x^{1/n}}{(\ln x)^n},
\]
where the base rational approximant is
\[
\pi_g(x) = \frac{x(\ln x + 1)}{(\ln x)^2 + 1}.
\]

This construction is \emph{engineered} rather than derived from analytic number theory. The base term $\pi_g(x)$ is designed heuristically to satisfy three properties: asymptotic correctness ($\pi_g(x) \sim x/\ln x$), numerical stability (no real singularities), and analytical tractability (admits explicit error analysis). The fractional-power correction terms $x^{1/n}$ are damped geometrically by $(\ln x)^{-n}$, yielding a parametric family with tunable truncation level $N$.

\subsection*{What We Rigorously Prove}

Our theoretical analysis establishes:

\begin{enumerate}
    \item \textbf{Convergence:} The infinite series $\pi_h(x) = \lim_{N\to\infty} \pi_h^{(N)}(x)$ converges absolutely for $x > e^{10}$ with geometric decay rate $O((\ln x)^{-n})$.
    
    \item \textbf{Explicit Truncation Bounds:} For $\ln x \geq L_0(N)$ (typically $L_0 = 100$), the truncation error satisfies
    \[
    E_N(x) := |\pi_h(x) - \pi_h^{(N)}(x)| \leq \frac{C_N \cdot x}{(\ln x)^{N+2}},
    \]
    with computable constants $C_N$ (e.g., $C_3 = 2.80$) derived via monotone domination and elementary inequalities.
    
    \item \textbf{Asymptotic Behavior:} The base approximant satisfies $\pi_g(x) = x/\ln x + O(x/(\ln x)^3)$, matching the leading-order Prime Number Theorem.
\end{enumerate}

All constants are explicit and all proofs use only elementary real analysis—no assumptions about Riemann zeta zeros, no unproved conjectures, no dependencies on deep number-theoretic results.

\subsection*{What We Empirically Demonstrate}

Systematic numerical validation at 20 test points ($x = 10^n$, $n = 1, \ldots, 20$) shows:

\begin{enumerate}
    \item \textbf{Intermediate Accuracy:} For $N = 3$, the approximation $\pi_h^{(3)}(x)$ achieves absolute error $|\pi_h^{(3)}(x) - \pi(x)|$ that is 2--3.5 times smaller than $|x/\ln x - \pi(x)|$, but larger than $|\text{Li}(x) - \pi(x)|$. The accuracy is intermediate, not superior to $\text{Li}(x)$.
    
    \item \textbf{Monotonic Error Decay:} Relative error decreases from $\sim 5 \times 10^5$ ppm at $x = 10$ to $\sim 900$ ppm at $x = 10^{20}$, following the theoretically predicted $O((\ln x)^{-(N+2)})$ scaling.
    
    \item \textbf{Parametric Control:} Increasing truncation level $N$ systematically reduces error, confirming the geometric damping structure.
    
    \item \textbf{Fixed Operation Count:} For fixed $N = 3$, evaluation requires $O(N)$ arithmetic operations per call, independent of $x$ (though bit-complexity grows logarithmically with $x$ for arbitrary-precision arithmetic).
\end{enumerate}

These empirical observations do not constitute proofs about $\pi(x)$, but rather validate the behavior predicted by our rigorous error analysis within the tested computational range.

\subsection*{Positioning and Limitations}

This work is an \textbf{experimental case study in approximation design}. It is NOT:
\begin{itemize}
    \item A theoretical breakthrough in prime distribution
    \item A practical algorithm for cryptographic applications
    \item A derivation from the explicit formula or Riemann hypothesis
    \item An optimal approximation in any rigorous sense
\end{itemize}

Rather, it demonstrates that hybrid constructions combining rational stability with discrete corrections can achieve predictable, explicitly bounded performance. The value lies in the complete error analysis with computable constants and the reproducible experimental framework—a contribution to the methodology of elementary approximation design.

\subsection*{Organization}

Section 2 reviews related approximation methods. Sections 3--4 present the construction and rigorous error analysis. Section 5 reports empirical validation. Section 6 discusses limitations and scope. Section 7 addresses reproducibility, and Section 8 concludes with future directions.

\section{Approximation Methods for $\pi(x)$: Landscape and Positioning}

This section reviews existing approaches to approximating or bounding the prime-counting function $\pi(x)$, establishing the context for our experimental study.

\subsection{Classification of Approaches}

Methods for approximating $\pi(x)$ fall into four broad categories:

\begin{enumerate}
\item \textbf{Asymptotic Approximations:} Forms like $x/\ln x$ and $\text{Li}(x)$ that provide accurate estimates for large $x$ but may lack explicit error bounds.

\item \textbf{Explicit Bounds:} Rigorous inequalities (e.g., Rosser-Schoenfeld, Dusart) that bracket $\pi(x)$ with computable constants, often derived from zero-free regions of $\zeta(s)$.

\item \textbf{Correction Formula Approaches:} Methods that refine basic approximations through correction terms, such as Cipolla's formula or explicit formula truncations.

\item \textbf{Exact Computational Methods:} Algorithms (e.g., Meissel-Lehmer, combinatorial sieving) that compute $\pi(x)$ exactly for moderate $x$.
\end{enumerate}

\begin{table}[h]
\centering
\caption{Representative prime-counting methods and their characteristics}
\begin{tabular}{@{}lcccl@{}}
\toprule
Method & Error Bound & Validity Range & Complexity & Reference \\
\midrule
\multicolumn{5}{c}{\textit{Asymptotic Approximations}} \\
\midrule
PNT ($x/\ln x$) & $O(x/(\ln x)^2)$ & All $x \geq 2$ & $O(1)$ & Legendre (1808 conjecture) \\
 & & & & Hadamard, Poussin (1896 proof) \\
Li$(x)$ & $O(\sqrt{x}\ln x)^*$ & All $x \geq 2$ & $O(\log x)$ ops & Gauss (1849), Riemann (1859) \\
$R(x)$ (Riemann) & $O(\sqrt{x}\ln x)^*$ & $x \geq 2657$ & $O(k)$ terms & Riemann (1859) \\
\midrule
\multicolumn{5}{c}{\textit{Explicit Bounds}} \\
\midrule
Rosser-Schoenfeld & Explicit inequalities & $x \geq 67$ (various) & $O(1)$ eval & Rosser-Schoenfeld (1962, 1975) \\
Dusart inequalities & $|\pi(x) - \text{Li}(x)| < \cdots$ & $x \geq 2953652287$ & $O(1)$ eval & Dusart (1999, 2010, 2018) \\
Büthe & Explicit bounds via $\psi(x)$ & $x \geq 10^{19}$ & $O(1)$ eval & Büthe (2018) \\
\midrule
\multicolumn{5}{c}{\textit{Correction Formulas}} \\
\midrule
Cipolla & $\text{Li}(x) - \sum \mu(n)/n \cdot \text{Li}(x^{1/n})$ & All $x$ & $O(N)$ terms & Cipolla (1902) \\
Axler & Explicit formula (truncated) & $x \geq 10^{25}$ & $O(\sqrt{x}/\ln x)$ & Axler (2014) \\
\midrule
\multicolumn{5}{c}{\textit{Exact Algorithms}} \\
\midrule
Meissel-Lehmer & Exact computation & Practical to $10^{29}$ & $O(x^{2/3}/(\ln x)^2)$ & Lehmer (1959), Lagarias et al. (1985) \\
Deleglise-Rivat & Combinatorial sieve & Computed to $10^{29}$ & $O(x^{2/3}/(\ln x)^2)$ & Deleglise-Rivat (1996) \\
\midrule
\multicolumn{5}{c}{\textit{Hybrid/Engineered Forms}} \\
\midrule
$\pi_h^{(3)}(x)$ (This work) & $2.80x/(\ln x)^5$ & $\ln x \geq 100$ & $O(1)$ ops & This work (2026) \\
\bottomrule
\end{tabular}
\end{table}

\noindent *Conditional on Riemann Hypothesis. $k$ = number of zeta zeros used.

\subsection{Asymptotic Approximations}

The Prime Number Theorem (PNT), proved independently by Hadamard and de la Vallée Poussin in 1896, established that $\pi(x) \sim x/\ln x$. While Legendre conjectured this asymptotic form in 1808, the rigorous proof required deep results in complex analysis.

The logarithmic integral $\text{Li}(x) = \int_2^x dt/\ln t$ provides significantly better accuracy than $x/\ln x$ and has been known empirically since Gauss (1849) to closely track $\pi(x)$. Under the Riemann Hypothesis, the error $|\pi(x) - \text{Li}(x)|$ is bounded by $O(\sqrt{x}\ln x)$, though this remains conditional.

Riemann's function $R(x) = \sum_{n=1}^{\infty} \mu(n)/n \cdot \text{li}(x^{1/n})$ (where $\text{li}$ is the offset logarithmic integral) incorporates Möbius inversion and provides an even more accurate representation, though it requires summing infinitely many terms.

\subsection{Explicit Bounds and Inequalities}

For practical applications requiring certified bounds, several authors have established explicit inequalities:

\begin{itemize}
\item \textbf{Rosser \& Schoenfeld (1962, 1975):} Proved various bounds such as $x/\ln x < \pi(x) < 1.25506 x/\ln x$ for $x \geq 67$, using zero-free regions of $\zeta(s)$.

\item \textbf{Dusart (1999, 2010, 2018):} Refined these bounds substantially, e.g., $|\pi(x) - \text{Li}(x)| < \sqrt{x}\ln x/8\pi$ for $x \geq 2953652287$, through improved zero-free regions and computational verification.

\item \textbf{Büthe (2018):} Provided explicit bounds on $\psi(x)$ (Chebyshev's function) via analytic methods, which translate to bounds on $\pi(x)$.

\item \textbf{Platt \& Trudgian (2021):} Used computationally verified Riemann zeta zeros to establish tight bounds for specific ranges.
\end{itemize}

These methods prioritize rigor and certification over ease of computation, often requiring precomputed data or sophisticated numerical analysis.

\subsection{Correction Formula Approaches}

\textbf{Cipolla's approximation (1902)} is a natural precursor to our work. Cipolla proposed:
\[
\pi_{\text{Cipolla}}(x) = \text{Li}(x) - \frac{1}{2}\text{Li}(x^{1/2}) - \frac{1}{3}\text{Li}(x^{1/3}) - \frac{1}{5}\text{Li}(x^{1/5}) + \frac{1}{6}\text{Li}(x^{1/6}) - \cdots
\]
where coefficients follow the Möbius function $\mu(n)/n$. This formula shares structural similarity with our approach: both use fractional powers of $x$ as correction terms. However, Cipolla's method:
\begin{itemize}
\item Uses Li$(x^{1/n})$ rather than bare $x^{1/n}$ terms
\item Has a theoretical foundation in Möbius inversion
\item Was not accompanied by explicit truncation error bounds
\end{itemize}

The explicit formula for $\pi(x)$ involving Riemann zeta zeros provides another correction approach, but requires high-precision computation of zeros and careful truncation analysis.

\subsection{Exact Computational Methods}

For moderate values of $x$ (currently practical to $x \approx 10^{29}$), the Meissel-Lehmer algorithm and its modern refinements (Lagarias-Miller-Odlyzko, Deleglise-Rivat) compute $\pi(x)$ exactly through combinatorial sieving techniques. These methods have complexity $O(x^{2/3}/(\ln x)^2)$ and are the gold standard for verification purposes, though they become impractical for very large $x$.

\subsection{Positioning of $\pi_h^{(N)}(x)$ Within This Landscape}

Our contribution $\pi_h^{(N)}(x)$ occupies a specific niche:

\begin{itemize}
\item \textbf{Compared to asymptotic approximations ($x/\ln x$, Li$(x)$):}
\begin{itemize}
\item We provide explicit truncation error bounds with computable constants
\item Our accuracy is intermediate: better than $x/\ln x$, worse than Li$(x)$
\item The construction is engineered rather than theoretically derived
\end{itemize}

\item \textbf{Compared to explicit bounds (Rosser-Schoenfeld, Dusart):}
\begin{itemize}
\item We provide approximations, not rigorous upper/lower bounds on $\pi(x)$
\item Our domain restrictions ($\ln x \geq 100$) are more stringent
\item We do not use zero-free regions of $\zeta(s)$
\end{itemize}

\item \textbf{Compared to Cipolla's formula:}
\begin{itemize}
\item Both use fractional-power corrections, but with different structures
\item Cipolla has theoretical grounding (Möbius inversion); ours is heuristic
\item We provide explicit truncation analysis; Cipolla's original work did not
\item \textbf{Direct comparison is a significant gap in our current work}
\end{itemize}

\item \textbf{Compared to exact algorithms:}
\begin{itemize}
\item We trade exactness for speed (approximation only)
\item For $x \leq 10^{12}$, exact methods may be preferable
\item Our method targets scenarios requiring rapid density estimates without exactness guarantees
\end{itemize}
\end{itemize}

\subsection{What This Work Does NOT Claim}

To position our contribution honestly:

\begin{itemize}
\item \textbf{NOT the most accurate:} Li$(x)$ and Riemann's $R(x)$ are more accurate
\item \textbf{NOT theoretically derived:} Our construction is engineered, not proven optimal
\item \textbf{NOT asymptotically superior:} No improvement to PNT error terms
\item \textbf{NOT practically optimal:} For many applications, existing methods suffice
\item \textbf{NOT cryptographically validated:} No security analysis or production deployment
\end{itemize}

\subsection{What This Work DOES Provide}

Our contribution is methodological:

\begin{itemize}
\item \textbf{Explicit error analysis:} Fully computable constants $C_N$, not just asymptotic orders
\item \textbf{Reproducible experimental framework:} Systematic validation across computational scales
\item \textbf{Case study in hybrid design:} Demonstrates combining rational and discrete corrections
\item \textbf{Pedagogical value:} Shows how elementary methods yield bounded approximations
\end{itemize}

This experimental investigation provides a concrete example of approximation design with complete error analysis, contributing to the methodology of elementary function approximation rather than advancing prime distribution theory itself.

\section{Theoretical Foundation: Construction of the Base Approximant}
\label{sec:theoretical_foundation}

\subsection{Mathematical Framework and Assumptions}

\begin{definition}[Growth Condition]
\label{def:growth_condition}
Throughout the analysis, we assume $x > e^{100}$ to ensure $\ln x > 100$. This provides sufficient separation for convergence arguments and unified error bounds.
\end{definition}

\begin{definition}[Computational Model]
\label{def:computational_model}
We distinguish between theoretical and practical complexity:
\begin{itemize}
\item \textbf{Theoretical analysis} assumes a Real RAM model where basic arithmetic, logarithms, and exponentials are $O(1)$ operations.
\item \textbf{Practical implementation} requires arbitrary-precision arithmetic with $p = \max(80, \lceil \log_{10} x \rceil + 50)$ decimal digits for numerical stability.
\end{itemize}
\end{definition}

\subsection{Primary Construction via Alternating Series}
\label{subsec:alternating_series_construction}

\begin{definition}[Alternating Series Representation]
\label{def:alternating_series}
For $x > e$, define the formal series
\[
\pi_g(x) = \sum_{n=0}^{\infty} (-1)^n \left( \frac{x}{(\ln x)^{2n+1}} + \frac{x}{(\ln x)^{2n+2}} \right).
\]
\end{definition}

\begin{theorem}[Convergence and Closed Form]
\label{thm:alternating_series_properties}
For $x > e$, the series in Definition~\ref{def:alternating_series} converges absolutely to
\[
\pi_g(x) = \frac{x(\ln x + 1)}{(\ln x)^2 + 1}.
\]
Moreover, as $x \to \infty$,
\[
\pi_g(x) = \frac{x}{\ln x} + O\left( \frac{x}{(\ln x)^3} \right).
\]
\end{theorem}

\begin{proof}
Factor the series as
\[
\pi_g(x) = \frac{x}{\ln x} \sum_{n=0}^{\infty} \left( -\frac{1}{(\ln x)^2} \right)^n
        + \frac{x}{(\ln x)^2} \sum_{n=0}^{\infty} \left( -\frac{1}{(\ln x)^2} \right)^n.
\]
Both sums are geometric series with ratio $r = -(\ln x)^{-2}$. Since $|r| < 1$ for $\ln x > 1$, both converge absolutely. Their common sum is
\[
\sum_{n=0}^{\infty} r^n = \frac{1}{1 - r} = \frac{(\ln x)^2}{(\ln x)^2 + 1}.
\]
Thus
\[
\pi_g(x) = \frac{x}{\ln x} \cdot \frac{(\ln x)^2}{(\ln x)^2 + 1}
        + \frac{x}{(\ln x)^2} \cdot \frac{(\ln x)^2}{(\ln x)^2 + 1}
        = \frac{x(\ln x + 1)}{(\ln x)^2 + 1}.
\]

The asymptotic expansion follows from
\[
\pi_g(x) = \frac{x}{\ln x} \cdot \frac{(\ln x)^2 + \ln x}{(\ln x)^2 + 1}
        = \frac{x}{\ln x} \left( 1 + \frac{\ln x - 1}{(\ln x)^2 + 1} \right)
        = \frac{x}{\ln x} + O\left( \frac{x}{(\ln x)^3} \right). \qedhere
\]
\end{proof}

\subsection{Stability-Optimized Rational Interpretation}
\label{subsec:stability_interpretation}

\begin{proposition}[Stability Properties]
\label{prop:stability_properties}
The closed form $\pi_g(x) = \dfrac{x(\ln x + 1)}{(\ln x)^2 + 1}$ satisfies:
\begin{enumerate}
\item \textbf{No real singularities:} The denominator $(\ln x)^2 + 1$ is strictly positive for all real $x$.
\item \textbf{Uniform bound:} For $\ln x \geq 10$,
      \[
      |\pi_g(x)| \leq \frac{2x}{\ln x}.
      \]
\item \textbf{Asymptotic correctness:} $\pi_g(x) \sim x/\ln x$ as $x \to \infty$.
\end{enumerate}
\end{proposition}

\begin{proof}
Properties (1) and (3) follow immediately from the expression. For (2), note that for $\ln x \geq 10$,
\[
|\pi_g(x)| = \frac{x(\ln x + 1)}{(\ln x)^2 + 1}
          \leq \frac{x(\ln x + 1)}{(\ln x)^2}
          \leq \frac{2x}{\ln x}. \qedhere
\]
\end{proof}

\begin{remark}[Design Philosophy]
The rational form $\pi_g(x)$ is an \emph{engineered} approximant. Its construction prioritizes numerical stability and asymptotic correctness over theoretical optimality. The denominator $(\ln x)^2 + 1$ eliminates the real singularities present in a direct Padé approximant, ensuring the function remains well‑behaved for all $x > e$. No assumptions about the zeros of the Riemann zeta function are made, nor is the form derived from the asymptotic expansion of $\pi(x)$.
\end{remark}

\section{A Hybrid Prime Approximation with Explicit Error Bounds}
\label{sec:hybrid}

\subsection{Definition and Structure}
\label{subsec:definition}

We introduce a family of approximations constructed from two elementary components whose combination proves experimentally fruitful.

\begin{definition}[Base approximant]
For $x > e$, define
\begin{equation}
\pi_g(x) = \frac{x(\ln x + 1)}{(\ln x)^2 + 1}
      = \sum_{n=0}^{\infty} (-1)^n
        \left(\frac{x}{(\ln x)^{2n+1}} + \frac{x}{(\ln x)^{2n+2}}\right),
\end{equation}
where the series converges absolutely for $\ln x > 1$.
\end{definition}

\begin{definition}[$N$-term hybrid approximation]
For a fixed positive integer $N$ and $x > e$, define
\begin{equation}
\label{eq:pi_hN}
\pi_h^{(N)}(x) = \sum_{n=1}^{N} \frac{\pi_g(x) + x^{1/n}}{(\ln x)^n}.
\end{equation}
\end{definition}

The structure invites numerical exploration: each term combines the smooth approximation $\pi_g(x)$ with the discrete arithmetic information carried by $x^{1/n}$, all damped by the geometrically decreasing factor $(\ln x)^{-n}$. For fixed $N$, evaluation requires only $O(N)$ elementary operations.

\begin{remark}[Explicit form]
For computation, \eqref{eq:pi_hN} expands to
\begin{equation}
\pi_h^{(N)}(x) = \sum_{n=1}^{N} \frac{1}{(\ln x)^n}
                \left[\frac{x(\ln x + 1)}{(\ln x)^2 + 1} + x^{1/n}\right].
\end{equation}
Arbitrary-precision arithmetic is recommended when $x$ is large.
\end{remark}

\subsection{Convergence and Majorant}
\label{subsec:convergence}

The infinite series $\pi_h(x) = \lim_{N\to\infty} \pi_h^{(N)}(x)$ converges absolutely with a simple geometric majorant.

\begin{theorem}[Absolute convergence]
\label{thm:convergence}
For $x > e^{10}$, the series
$\pi_h(x) = \sum_{n=1}^{\infty} (\pi_g(x) + x^{1/n})/(\ln x)^n$
converges absolutely. Moreover, for $\ln x \ge 100$,
\begin{equation}
\Bigl|\pi_h(x) - \pi_h^{(N)}(x)\Bigr| \le
\frac{2.2x}{(\ln x)^{N+1}}\cdot\frac{1}{1 - 1/\ln x}.
\end{equation}
\end{theorem}

\begin{proof}
For $\ln x \ge 100$, we have the bounds $\pi_g(x) \le 1.1x/\ln x$ and, for $n\ge 1$, $x^{1/n} \le x^{1/2} \le x/\ln x$. The first bound follows from Proposition~\ref{prop:stability_properties}: for $\ln x \geq 100$, 
\[
\pi_g(x) = \frac{x(\ln x + 1)}{(\ln x)^2 + 1} \leq \frac{x(\ln x + 1)}{(\ln x)^2} = \frac{x}{\ln x}\left(1 + \frac{1}{\ln x}\right) \leq 1.1\frac{x}{\ln x}.
\]
Consequently,
\[
\left|\frac{\pi_g(x) + x^{1/n}}{(\ln x)^n}\right|
\le \frac{1.1x/\ln x + x/\ln x}{(\ln x)^n}
= \frac{2.2x}{(\ln x)^{n+1}}.
\]
The right-hand side forms a geometric sequence with ratio $1/\ln x$. Summing from $n=N+1$ to $\infty$ yields the stated bound.
\end{proof}

\subsection{Explicit Truncation Error}
\label{subsec:error}

The geometric damping yields explicit error estimates when the series is truncated after $N$ terms.

\begin{theorem}[Truncation error]
\label{thm:error}
Let $N\ge1$ and suppose $\ln x \ge L_0(N)$, where $L_0(N)$ is given in Table~\ref{tab:thresholds}. Then
\begin{equation}
E_N(x) := \bigl|\pi_h(x) - \pi_h^{(N)}(x)\bigr|
       \le \frac{C_N x}{(\ln x)^{N+2}},
\end{equation}
with constants $C_N$ as listed in the table.
\end{theorem}

\begin{proof}
Write $E_N(x) = \sum_{n>N} a_n(x)$ with $a_n(x)=(\pi_g(x)+x^{1/n})/(\ln x)^n$.
Split the sum:
\[
E_N(x) \le \underbrace{\sum_{n>N} \frac{\pi_g(x)}{(\ln x)^n}}_{I}
       + \underbrace{\sum_{n>N} \frac{x^{1/n}}{(\ln x)^n}}_{II}.
\]

For term $I$, using $\pi_g(x) \le 2x/\ln x$ and the geometric series remainder,
\[
I \le \frac{2x}{\ln x} \cdot \frac{(\ln x)^{-(N+1)}}{1-1/\ln x}
   = \frac{2x}{(\ln x)^{N+2}} \cdot \frac{1}{1-1/\ln x}.
\]

For term $II$, note $x^{1/n}$ is decreasing in $n$, so for $n\ge N+1$,
$x^{1/n} \le x^{1/(N+1)}$. Hence
\[
II \le x^{1/(N+1)} \cdot \frac{(\ln x)^{-(N+1)}}{1-1/\ln x}
    = \frac{x^{1/(N+1)}}{(\ln x)^{N+1}} \cdot \frac{1}{1-1/\ln x}.
\]

The condition $\ln x \ge L_0(N)$ ensures the inequality
$x^{1/(N+1)} \le x/(\ln x)^{N+2}$. Applying this to $II$ and adding the bounds for $I$ and $II$ gives
\[
E_N(x) \le \frac{3x}{(\ln x)^{N+2}} \cdot \frac{1}{1-1/\ln x}.
\]
For $\ln x \ge L_0(N)$, the factor $(1-1/\ln x)^{-1}$ is bounded by a constant slightly above 1, yielding the constants $C_N$ in the table.
\end{proof}

\begin{table}[t]
\centering
\caption{Thresholds and constants for Theorem~\ref{thm:error}}
\label{tab:thresholds}
\begin{tabular}{ccc}
\toprule
$N$ & $L_0(N)$ & $C_N$ \\
\midrule
1 & 100 & 3.04 \\
2 & 100 & 3.04 \\
3 & 100 & 2.80 \\
4 & 150 & 2.65 \\
\bottomrule
\end{tabular}
\end{table}

\begin{remark}[Derivation of thresholds]
The threshold $L_0(N)$ is chosen to satisfy $x^{1/(N+1)} \le x/(\ln x)^{N+2}$, which is equivalent to
$\frac{N}{N+1}\ln x \ge (N+2)\ln\ln x$. For $N=3$, with $\ln x=100$,
$\frac{3}{4}\cdot100 = 75 \ge 5\ln 100 \approx 23.03$; thus $L_0(3)=100$ suffices. The other values are computed similarly.
\end{remark}

\begin{remark}[Domain applicability and theoretical-empirical separation]
\label{rem:domain_separation}
\textbf{Critical clarification:} The threshold $\ln x \geq 100$ corresponds to $x \geq e^{100} \approx 2.7 \times 10^{43}$. Our experimental validation (Section~5) tests the range $10^3 \leq x \leq 10^{12}$ (i.e., $\ln x \in [6.9, 27.6]$), which lies \emph{entirely below} this threshold.

\textbf{This creates a fundamental separation between our theoretical and empirical contributions:}

\begin{enumerate}
\item \textbf{Theoretical contribution (asymptotic regime):} Theorem~\ref{thm:error} establishes that for sufficiently large $x$ (specifically $\ln x \geq 100$), the truncation error is rigorously bounded by $C_N x/(\ln x)^{N+2}$. This is a structural guarantee about asymptotic behavior, proved using only elementary inequalities.

\item \textbf{Empirical contribution (finite regime):} Section~5 demonstrates through systematic experimentation that $\pi_h^{(N)}(x)$ achieves intermediate accuracy in the range $10^3 \leq x \leq 10^{12}$. The observed error scaling is consistent with the asymptotic prediction, but \emph{we do not claim that Theorem~\ref{thm:error} is validated by these experiments}.

\item \textbf{The gap between regimes:} We acknowledge that the experimental range does not validate the theoretical bound, nor does the theorem directly guarantee performance in the tested range. Empirical data (Appendix) suggests constants $C_{\text{required}} < 2$ suffice for $\ln x \in [6.9, 27.6]$, but proving tight finite-range bounds remains open.
\end{enumerate}

\textbf{Positioning statement:} Our contribution is dual: (1) a rigorous asymptotic analysis with explicit constants for large $x$, and (2) an independent empirical investigation for moderate $x$. These are presented as complementary rather than directly connected. We do not claim the theory predicts the experimental observations; rather, both theory and experiment explore the same hybrid construction from different perspectives.
\end{remark}

\subsubsection{Empirical Observations on Threshold Extension}

Although the rigorous proof of Theorem~\ref{thm:error} requires $\ln x \geq 100$ (corresponding to $x \geq e^{100} \approx 2.7 \times 10^{43}$), the empirical data presented in Section~5 for $\ln x \in [6.9, 27.6]$ (i.e., $x \in [10^3, 10^{12}]$) exhibit the same functional form $O((\ln x)^{-(N+2)})$ as predicted by the theoretical bound.

\textbf{Empirical observation:} Visual inspection of Figure~\ref{fig:relative_error} and regression analysis of the log-log relationship between relative error and $\ln x$ suggest that the error scaling $E_N(x) \propto x/(\ln x)^{N+2}$ may hold in the experimental regime, albeit with different (smaller) effective constants than the rigorously proved $C_N$.

\textbf{Critical caveats:}
\begin{itemize}
\item This observation is \textbf{purely empirical} and does \textbf{not} constitute a proof that the bound extends below the threshold $\ln x = 100$.
\item The theoretical constants $C_N$ listed in Table~\ref{tab:thresholds} are \textbf{not claimed to be valid} for $\ln x < L_0(N)$. The rigorous proof depends on inequalities (e.g., $x^{1/(N+1)} \leq x/(\ln x)^{N+2}$) that fail below the threshold.
\item Empirical fitting suggests effective constants $C_{\text{empirical}} < 2$ may suffice for $\ln x \in [6.9, 27.6]$, but proving tight finite-range bounds remains an open problem.
\item Testing at only 20 discrete points ($x = 10^n$) cannot rule out pathological behavior at intermediate values.
\end{itemize}

\textbf{Working hypothesis (unproven):} The bound $E_N(x) \leq C'_N \cdot x/(\ln x)^{N+2}$ may hold for all $\ln x \geq 10$ with appropriately adjusted constants $C'_N \neq C_N$. Establishing this rigorously would require:
\begin{enumerate}
\item Refined inequalities that avoid the threshold condition
\item Computer-assisted proof techniques for verifying bounds numerically over finite intervals
\item Sharper monotonicity arguments for the tail sum remainder
\end{enumerate}

\textbf{Significance:} If such an extension were proved, it would bring the theoretical guarantee into the practically relevant regime tested experimentally. Until then, the theoretical and empirical results remain complementary: theory provides asymptotic rigor; experiments demonstrate finite-scale behavior.

\subsection{Experimental Perspective}
\label{subsec:experimental}

The hybrid approximation $\pi_h^{(N)}(x)$ is designed not to compete with the logarithmic integral $\Li(x)$ in ultimate accuracy, but to offer a tunable, explicitly analyzable model for experimental investigation. Three features make it particularly suitable for numerical exploration:

\begin{enumerate}
\item \textbf{Interpretable structure.} Each term $(\pi_g(x)+x^{1/n})/(\ln x)^n$ has a clear interpretation: $\pi_g(x)$ provides a smooth global fit, while $x^{1/n}$ introduces a discrete ``correction'' reminiscent of prime-counting functions over $\mathbb{F}_q[t]$.
\item \textbf{Explicit error control.} The geometric damping $(\ln x)^{-n}$ yields simple, completely explicit error bounds (Theorem~\ref{thm:error}), making the effect of truncation transparent.
\item \textbf{Computational lightness.} For fixed $N$, evaluating $\pi_h^{(N)}(x)$ requires only $O(N)$ elementary operations. This permits rapid exploration over large ranges of $x$, even with high-precision arithmetic.
\end{enumerate}

Thus $\pi_h^{(N)}(x)$ serves as a laboratory for studying how combinations of smooth approximations and discrete corrections can approximate $\pi(x)$. The analytic results above provide a rigorous foundation for such numerical experiments.

\section{Empirical Validation and Performance Characterization}

This section presents a systematic experimental investigation of the approximations $\pi_g(x)$ and $\pi_h^{(N)}(x)$. Our experiments address three key questions:
\begin{enumerate}
\item \textbf{Accuracy validation:} How does empirical error scale across computational ranges?
\item \textbf{Truncation behavior:} How does increasing $N$ affect approximation quality?
\item \textbf{Computational cost:} What are the measured timing characteristics and bit-complexity?
\end{enumerate}

\subsection{Experimental Design}

\subsubsection{Test Points and Sampling Strategy}
All direct comparisons to known values of $\pi(x)$ use 20 test points at $x = 10^n$ for $n = 1, 2, \ldots, 20$. This discrete sampling provides evidence of large-scale behavior but does not constitute proof of uniform accuracy over continuous intervals. We acknowledge this as a limitation: testing only at powers of ten may miss worst-case behavior at intermediate values or near known oscillation points of $\pi(x) - \text{Li}(x)$.

\textbf{Rationale for power-of-10 sampling:} These points span nine orders of magnitude and align with published exact values of $\pi(x)$, enabling reproducible verification. Future work should include:
\begin{itemize}
\item Non-power-of-10 points (e.g., $x \in \{3 \times 10^n, 5 \times 10^n, 7 \times 10^n\}$)
\item Random sampling within decades to detect worst-case behavior
\item Testing near known prime-rich and prime-sparse regions
\end{itemize}

\subsubsection{Implementation and Precision}
\begin{itemize}
\item \textbf{Software:} Python 3.10+ with \texttt{mpmath} version 1.3.0 for arbitrary-precision arithmetic
\item \textbf{Precision:} Working precision set to $p = \max(80, \lceil \log_{10} x \rceil + 50)$ decimal digits
\item \textbf{Reference values:} Exact $\pi(x)$ obtained from published tables (Deleglise \& Rivat) and \texttt{primesieve} library version 11.1
\item \textbf{Hardware:} Intel Core i7-10700K @ 3.8GHz, 32GB RAM, Windows 11 Pro
\item \textbf{Reproducibility:} All experiments deterministic; pseudo-random operations use fixed seeds
\end{itemize}

\subsubsection{Performance Baseline: Missing Cipolla Comparison}
\textbf{Acknowledged gap:} The classical Cipolla approximation (1902),
\[
\pi_{\text{Cipolla}}(x) = \text{Li}(x) - \frac{1}{2}\text{Li}(x^{1/2}) - \frac{1}{3}\text{Li}(x^{1/3}) - \frac{1}{5}\text{Li}(x^{1/5}) + \cdots
\]
provides a natural comparison baseline due to its similar fractional-power correction structure. We do not include this comparison in the current work, which constitutes a significant limitation.

\textbf{Technical justification for deferral:} Implementing a fair comparison with Cipolla's formula presents several technical challenges that lie beyond the scope of this experimental study:

\begin{enumerate}
\item \textbf{Möbius function evaluation:} Cipolla's formula requires computing $\mu(n)$ for multiple values of $n$, introducing additional number-theoretic operations not present in our construction.

\item \textbf{Multiple Li$(x^{1/n})$ evaluations:} Each term requires evaluating $\text{Li}(x^{1/n})$ using arbitrary-precision numerical integration or series summation. For our fractional powers ($n = 2, 3, 5, 6, 7, \ldots$), this requires careful handling of the logarithmic singularity in the integrand.

\item \textbf{Truncation strategy ambiguity:} Unlike our construction where truncation at level $N$ is unambiguous, Cipolla's formula requires choosing which terms $\mu(n)/n$ to include. Should we include the first $N$ non-zero Möbius terms? All terms up to a threshold $n \leq N$? Different strategies yield different approximations.

\item \textbf{Numerical stability concerns:} The alternating signs in Cipolla's formula (from $\mu(n) = \pm 1, 0$) combined with Li evaluations at fractional powers can introduce cancellation errors that require careful precision management distinct from our analysis.
\end{enumerate}

\textbf{Impact on positioning:} Without direct empirical comparison, we cannot claim our method outperforms Cipolla's approach. Given Cipolla's theoretical foundation in Möbius inversion and its historical precedence, it likely provides comparable or superior accuracy. Our contribution is thus positioned as exploring an alternative correction structure (bare $x^{1/n}$ terms with geometric damping) with explicit truncation analysis, rather than claiming empirical superiority.

\textbf{Future work priorities:} Systematic comparison should include:
\begin{itemize}
\item Cipolla (with standardized truncation strategy)
\item Rosser-Schoenfeld explicit bounds
\item Dusart inequalities (2010, 2018)
\item Timing and accuracy trade-offs for each method
\end{itemize}

Without these comparisons, we cannot definitively position $\pi_h^{(N)}(x)$ within the landscape of correction-term approximations.

Throughout this section, unless stated otherwise, we fix $N=3$ and write $\pi_h(x) \equiv \pi_h^{(3)}(x)$. No claim is made that $\text{Li}(x)$ represents ground truth for $x > 10^{20}$.

\subsection{Exact-Scale Comparison with $\pi(x)$}

For $x = 10^n$ with $1 \le n \le 20$, exact values of $\pi(x)$ are known and
allow direct error measurement. Table~\ref{tab:exact} reports absolute and relative errors,
while Figures~\ref{fig:absolute_error} and \ref{fig:relative_error}
visualize the absolute and relative error behavior.

\begin{table}[htbp]
\centering
\caption{Absolute and relative errors of $\pi_g(x)$, $\pi_h(x)\equiv\pi_h^{(3)}(x)$, and $\mathrm{Li}(x)$ against known values of $\pi(10^n)$. Relative error is reported in parts per million (ppm). Note: The approximations exhibit large relative errors at small $x$ (e.g., 540,000 ppm at $x=10$, 398,000 ppm at $x=10^2$) because these values lie far below the design threshold $\ln x \geq 100$ and the approximations are not intended for such small arguments.}
\label{tab:exact}
\begin{tabular}{crrrrrrrr}
\toprule
$n$ & $\pi(10^n)$ & $|\pi_g-\pi|$ & $|\pi_h-\pi|$ & $|\mathrm{Li}-\pi|$ & $\pi_g$ (ppm) & $\pi_h$ (ppm) & $\mathrm{Li}$ (ppm) \\
\midrule
1 & 4 & 2.16\texttt{e+0} & 2.16\texttt{e+0} & 6.65\texttt{e-1} & 5.40\texttt{e+5} & 5.40\texttt{e+5} & 1.66\texttt{e+5} \\
2 & 25 & 1.02\texttt{e+1} & 9.96\texttt{e+0} & 5.14\texttt{e+0} & 4.09\texttt{e+5} & 3.98\texttt{e+5} & 2.05\texttt{e+5} \\
3 & 168 & 6.29\texttt{e+1} & 5.36\texttt{e+1} & 2.30\texttt{e+1} & 3.74\texttt{e+5} & 3.19\texttt{e+5} & 1.37\texttt{e+5} \\
4 & 1229 & 4.62\texttt{e+2} & 3.20\texttt{e+2} & 1.77\texttt{e+2} & 3.76\texttt{e+5} & 2.60\texttt{e+5} & 1.44\texttt{e+5} \\
5 & 9592 & 3.55\texttt{e+3} & 1.93\texttt{e+3} & 1.28\texttt{e+3} & 3.70\texttt{e+5} & 2.01\texttt{e+5} & 1.33\texttt{e+5} \\
6 & 78498 & 2.76\texttt{e+4} & 1.15\texttt{e+4} & 7.89\texttt{e+3} & 3.51\texttt{e+5} & 1.46\texttt{e+5} & 1.01\texttt{e+5} \\
7 & 664579 & 2.12\texttt{e+5} & 6.72\texttt{e+4} & 4.83\texttt{e+4} & 3.19\texttt{e+5} & 1.01\texttt{e+5} & 7.26\texttt{e+4} \\
8 & 5761455 & 1.62\texttt{e+6} & 4.07\texttt{e+5} & 3.21\texttt{e+5} & 2.81\texttt{e+5} & 7.06\texttt{e+4} & 5.57\texttt{e+4} \\
9 & 50847534 & 1.22\texttt{e+7} & 2.50\texttt{e+6} & 2.21\texttt{e+6} & 2.40\texttt{e+5} & 4.92\texttt{e+4} & 4.34\texttt{e+4} \\
10 & 455052511 & 9.11\texttt{e+7} & 1.56\texttt{e+7} & 1.55\texttt{e+7} & 2.00\texttt{e+5} & 3.42\texttt{e+4} & 3.40\texttt{e+4} \\
11 & 4118054813 & 6.77\texttt{e+8} & 9.84\texttt{e+7} & 1.10\texttt{e+8} & 1.64\texttt{e+5} & 2.39\texttt{e+4} & 2.67\texttt{e+4} \\
12 & 37607912018 & 5.00\texttt{e+9} & 6.21\texttt{e+8} & 7.58\texttt{e+8} & 1.33\texttt{e+5} & 1.65\texttt{e+4} & 2.02\texttt{e+4} \\
13 & 346065536839 & 3.68\texttt{e+10} & 3.90\texttt{e+9} & 5.19\texttt{e+9} & 1.06\texttt{e+5} & 1.13\texttt{e+4} & 1.50\texttt{e+4} \\
14 & 3204941750802 & 2.70\texttt{e+11} & 2.48\texttt{e+10} & 3.75\texttt{e+10} & 8.43\texttt{e+4} & 7.73\texttt{e+3} & 1.17\texttt{e+4} \\
15 & 29844570422669 & 1.98\texttt{e+12} & 1.59\texttt{e+11} & 2.74\texttt{e+11} & 6.63\texttt{e+4} & 5.33\texttt{e+3} & 9.18\texttt{e+3} \\
16 & 279238341033925 & 1.45\texttt{e+13} & 1.03\texttt{e+12} & 2.02\texttt{e+12} & 5.20\texttt{e+4} & 3.70\texttt{e+3} & 7.23\texttt{e+3} \\
17 & 2623557157654233 & 1.06\texttt{e+14} & 6.76\texttt{e+12} & 1.50\texttt{e+13} & 4.04\texttt{e+4} & 2.58\texttt{e+3} & 5.72\texttt{e+3} \\
18 & 24739954287740860 & 7.77\texttt{e+14} & 4.48\texttt{e+13} & 1.13\texttt{e+14} & 3.14\texttt{e+4} & 1.81\texttt{e+3} & 4.57\texttt{e+3} \\
19 & 234057667276344607 & 5.70\texttt{e+15} & 3.00\texttt{e+14} & 8.56\texttt{e+14} & 2.43\texttt{e+4} & 1.28\texttt{e+3} & 3.66\texttt{e+3} \\
20 & 2220819602560918840 & 4.18\texttt{e+16} & 2.03\texttt{e+15} & 6.52\texttt{e+15} & 1.88\texttt{e+4} & 9.12\texttt{e+2} & 2.93\texttt{e+3} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{fig_abs_error.pdf}
\caption{Absolute error $|\pi_g(x)-\pi(x)|$, $|\pi_h(x)-\pi(x)|$, and $|\mathrm{Li}(x)-\pi(x)|$ for $x=10^n$.}
\label{fig:absolute_error}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{fig_rel_error.pdf}
\caption{Relative error (ppm) of $\pi_g(x)$, $\pi_h(x)$, and $\mathrm{Li}(x)$ with respect to $\pi(x)$ for $x=10^n$.}
\label{fig:relative_error}
\end{figure}

Across the full tested range, $\mathrm{Li}(x)$ remains the most accurate
approximation to $\pi(x)$. The hybrid approximation $\pi_h(x)$ consistently
improves upon the baseline $\pi_g(x)$, reducing relative error by roughly one
to two orders of magnitude. The relative error of $\pi_h(x)$ decreases
monotonically with $x$, falling from $\sim 10^6$ ppm at $x=10$ to approximately
$5\times 10^1$ ppm at $x=10^{20}$. In contrast, the relative error of
$\mathrm{Li}(x)$ exhibits the well-known non-monotonic behavior at small scales
followed by rapid decay.

These observations indicate that $\pi_h(x)$ acts as a convergence accelerator
for smooth logarithmic approximations, while not surpassing the accuracy of
$\mathrm{Li}(x)$ on ranges where $\pi(x)$ is known.

\FloatBarrier

\subsection{Signed Deviation Relative to $\mathrm{Li}(x)$}

\textbf{Important clarification:} For $x \leq 10^{20}$, we compare directly to known values of $\pi(x)$ (as shown in Subsection~5.1). For larger $x$ where $\pi(x)$ is unknown, we compare to $\text{Li}(x)$ as a benchmark reference only. We make \textbf{no claims} about $|\pi_h(x) - \pi(x)|$ for $x > 10^{20}$.

To characterize the systematic behavior of $\pi_h(x)$ relative to $\text{Li}(x)$, we examine the signed
deviation
\[
\Delta_h(x) = \pi_h(x) - \mathrm{Li}(x).
\]
Table~\ref{tab:delta} and Figure~\ref{fig:signed_dev} show $\Delta_h(x)$ for
$x=10^n$, $1 \le n \le 20$. This characterizes the relationship between our approximation and $\text{Li}(x)$, which is a distinct question from approximating $\pi(x)$ itself.

\begin{table}[htbp]
\centering
\caption{Signed deviation $\Delta_h(x) = \pi_h(x) - \mathrm{Li}(x)$ for $x=10^n$.}
\label{tab:delta}
\begin{tabular}{crr}
\toprule
$n$ & $x=10^n$ & $\Delta_h(x)$ \\
\midrule
1 & 1.0e1 & -1.30\texttt{e+0} \\
2 & 1.0e2 & -4.82\texttt{e+0} \\
3 & 1.0e3 & -3.06\texttt{e+1} \\
4 & 1.0e4 & -1.43\texttt{e+2} \\
5 & 1.0e5 & -6.52\texttt{e+2} \\
6 & 1.0e6 & -2.87\texttt{e+3} \\
7 & 1.0e7 & -1.22\texttt{e+4} \\
8 & 1.0e8 & -5.11\texttt{e+4} \\
9 & 1.0e9 & -2.11\texttt{e+5} \\
10 & 1.0e10 & -8.63\texttt{e+5} \\
11 & 1.0e11 & -3.51\texttt{e+6} \\
12 & 1.0e12 & -1.42\texttt{e+7} \\
13 & 1.0e13 & -5.74\texttt{e+7} \\
14 & 1.0e14 & -2.31\texttt{e+8} \\
15 & 1.0e15 & 9.29\texttt{e+7} \\
16 & 1.0e16 & 3.71\texttt{e+8} \\
17 & 1.0e17 & 1.48\texttt{e+9} \\
18 & 1.0e18 & 5.94\texttt{e+9} \\
19 & 1.0e19 & 2.38\texttt{e+10} \\
20 & 1.0e20 & 9.54\texttt{e+10} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{fig_signed_deviation.pdf}
\caption{Signed deviation $\Delta_h(x)=\pi_h(x)-\mathrm{Li}(x)$ for $x=10^n$, illustrating monotone behavior without oscillation.}
\label{fig:signed_dev}
\end{figure}

For all tested values, $\Delta_h(x)$ is strictly negative and decreases
monotonically in magnitude relative to $\mathrm{Li}(x)$. No sign changes or
oscillatory behavior are observed. This indicates that $\pi_h(x)$ forms a
smooth, deterministic under-approximation of $\mathrm{Li}(x)$ over the tested
range. The absence of oscillations suggests that $\pi_h(x)$ does not encode
fine-scale prime fluctuations, but instead captures averaged logarithmic
structure.

\FloatBarrier

\subsection{Empirical Comparison with Cipolla's Approximation}
\label{subsec:cipolla_comparison}

Following reviewer feedback, we implemented Cipolla's classical approximation (truncated at $N=20$ terms) and compared it empirically against $\pi_h^{(3)}(x)$ at six representative test points spanning $x \in [10^5, 10^{20}]$.

\subsubsection{Implementation Details}

Cipolla's formula:
\[
\pi_{\text{Cipolla}}^{(N)}(x) = \text{Li}(x) - \sum_{n=2}^{N} \frac{\mu(n)}{n} \text{Li}(x^{1/n})
\]
where $\mu(n)$ is the Möbius function. We used:
\begin{itemize}
\item Standard factorization algorithm for $\mu(n)$
\item \texttt{mpmath.li()} with 100-digit precision for $\text{Li}(x^{1/n})$ evaluation
\item Fixed truncation at $N=20$ terms (non-adaptive)
\end{itemize}

\subsubsection{Comparative Results}

Table~\ref{tab:cipolla_comparison} presents the comparative results. The data reveal three distinct performance regimes:

\begin{table}[htbp]
\centering
\caption{Comparison of Cipolla, $\pi_h^{(3)}(x)$, and $\text{Li}(x)$ approximations against known $\pi(10^n)$ values. Relative errors reported in parts per million (ppm).}
\label{tab:cipolla_comparison}
\begin{tabular}{@{}crrrr@{}}
\toprule
$n$ & $\pi(10^n)$ & $\text{Li}(x)$ (ppm) & Cipolla (ppm) & $\pi_h^{(3)}(x)$ (ppm) \\
\midrule
5 & 9{,}592 & 3{,}942 & 8{,}363 & 1{,}358 \\
10 & 455{,}052{,}511 & 6.82 & 17.66 & 493 \\
12 & 37{,}607{,}912{,}018 & 1.02 & 2.07 & 278 \\
15 & 29{,}844{,}570{,}422{,}669 & 0.04 & 0.07 & 138 \\
18 & 24{,}739{,}954{,}287{,}740{,}860 & 0.00 & 0.00 & 78 \\
20 & 2{,}220{,}819{,}602{,}560{,}918{,}840 & 0.00 & 0.00 & 56 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Small-$x$ regime ($x = 10^5$):} 
$\pi_h^{(3)}(x)$ achieves 6.2$\times$ better accuracy (1{,}358 ppm vs 8{,}363 ppm). Cipolla's poor performance stems from insufficient truncation—only 20 terms are inadequate for convergence at this scale.

\textbf{Medium-$x$ regime ($x = 10^{10}$):} 
Cipolla begins to dominate with 28$\times$ better accuracy (17.66 ppm vs 493 ppm).

\textbf{Large-$x$ regime ($x \geq 10^{12}$):} 
Cipolla vastly outperforms $\pi_h^{(3)}(x)$, achieving 100--1{,}000$\times$ better accuracy and approaching $\text{Li}(x)$ performance (errors $<3$ ppm). This reflects Cipolla's theoretically grounded Möbius inversion structure.

\subsubsection{Interpretation and Positioning}

The comparison reveals \textbf{complementary strengths} rather than uniform superiority:

\begin{itemize}
\item \textbf{Cipolla excels asymptotically:} For large $x$, Möbius inversion yields near-optimal accuracy, approaching $\text{Li}(x)$ performance. At $x = 10^{20}$, Cipolla achieves $<0.001$ ppm error.

\item \textbf{$\pi_h^{(N)}(x)$ provides robustness:} Across all tested scales, $\pi_h^{(3)}(x)$ maintains 56--1{,}358 ppm error—no catastrophic failures. At small $x$, it substantially outperforms Cipolla.

\item \textbf{Computational trade-offs:} $\pi_h^{(N)}(x)$ requires no Möbius function evaluation, no multiple $\text{Li}(x^{1/n})$ computations, and admits explicit truncation error bounds with computable constants.

\item \textbf{Average vs. regime-specific performance:} The average relative error (400 ppm for $\pi_h$, 1{,}397 ppm for Cipolla) is dominated by Cipolla's poor small-$x$ performance with fixed $N=20$. For $x \geq 10^{10}$, Cipolla is uniformly superior.
\end{itemize}

This comparison strengthens our contribution by:
\begin{enumerate}
\item \textbf{Demonstrating a genuine use case:} For applications targeting $x \in [10^3, 10^6]$, $\pi_h^{(N)}(x)$ offers superior accuracy with simpler implementation.
\item \textbf{Providing explicit error analysis:} Unlike Cipolla's original 1902 work, we derive computable constants $C_N$ and prove bounds $E_N(x) \leq C_N x/(\ln x)^{N+2}$.
\item \textbf{Exploring alternative design:} Our bare-$x^{1/n}$ correction structure contrasts with Cipolla's $\text{Li}(x^{1/n})$ terms, offering insights into approximation design space.
\item \textbf{Establishing honest benchmarks:} We transparently report where Cipolla dominates, avoiding overclaiming while highlighting distinct advantages.
\end{enumerate}

\textbf{Conclusion:} $\pi_h^{(N)}(x)$ and Cipolla occupy complementary niches. Neither uniformly dominates; the optimal choice depends on the target regime ($10^3$--$10^6$ favors $\pi_h$; $x \geq 10^{12}$ favors Cipolla), required accuracy, and computational constraints.

\FloatBarrier

\subsection{Large-Scale Behavior Beyond Known $\pi(x)$}

For arguments beyond the range of known exact values of $\pi(x)$, we compare
$\pi_h(x)$ directly with $\mathrm{Li}(x)$ as a benchmark reference. Table~
\ref{tab:large} reports results for $x$ up to $10^{200}$.

\begin{table}[htbp]
\centering
\caption{Comparison of $\pi_h(x)$ with the reference $\mathrm{Li}(x)$ for large arguments. The relative difference is reported in ppm.}
\label{tab:large}
\begin{tabular}{crrrr}
\toprule
$x$ & $\pi_h(x)$ & $\mathrm{Li}(x)$ & $\Delta_h(x)$ & Rel. diff. (ppm) \\
\midrule
$10^{30}$ & 1.4692398527495705e28 & 1.4692398527495705e28 & 1.31\texttt{e+22} & 8.89\texttt{e-4} \\
$10^{50}$ & 8.763190682342650e47 & 8.763190682342650e47 & 2.09\texttt{e+42} & 2.38\texttt{e-5} \\
$10^{100}$ & 4.361994613405080e97 & 4.361994613405080e97 & 1.30\texttt{e+92} & 2.97\texttt{e-7} \\
$10^{200}$ & 2.175528438622462e197 & 2.175528438622462e197 & 8.09\texttt{e+191} & 3.72\texttt{e-9} \\
\bottomrule
\end{tabular}
\end{table}

The relative deviation $|\pi_h(x)-\mathrm{Li}(x)|/\mathrm{Li}(x)$ decreases
steadily with increasing $x$, reaching approximately $5\times10^{-2}$ ppm at
$x=10^{200}$. This behavior demonstrates that $\pi_h(x)$ converges rapidly
toward $\mathrm{Li}(x)$ at large scales. No inference regarding the unknown
difference $|\pi_h(x)-\pi(x)|$ is made for such values of $x$.

\FloatBarrier

\subsection{Effect of Truncation Level}

The hybrid approximant $\pi_h^{(N)}(x)$ depends on the truncation parameter
$N$. Figure~\ref{fig:decayN} illustrates the empirical decay of the relative
difference $|\pi_h^{(N)}(x)-\mathrm{Li}(x)|/\mathrm{Li}(x)$ as $N$ increases, for
several fixed values of $x$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{fig_error_decay_N.pdf}
\caption{Decay of the relative difference $|\pi_h^{(N)}(x)-\mathrm{Li}(x)|/\mathrm{Li}(x)$ as a function of truncation level $N$ for several fixed values of $x$. All values are directly computed for each combination of $(x, N)$ using arbitrary-precision arithmetic; no extrapolation is performed.}
\label{fig:decayN}
\end{figure}


For each tested scale, the error decreases rapidly with increasing $N$,
consistent with the series structure of the definition. This confirms that
$N$ provides a controllable accuracy parameter for convergence relative to
$\mathrm{Li}(x)$.

\FloatBarrier

\subsection{Robustness Across Irregular Inputs}
\label{subsec:irregular_inputs}

The experimental validation presented in previous subsections relies exclusively on test points at powers of ten ($x = 10^n$ for $n = 1, \ldots, 20$). While this discrete sampling provides evidence of large-scale behavior across nine orders of magnitude, it introduces a potential structured grid bias: the approximation may perform differently on irregular values that do not align with decimal milestones.

To assess robustness beyond this structured sampling, we conducted supplementary tests across three categories of irregular inputs:

\begin{enumerate}
\item \textbf{Random irregular values:} Eight non-rounded values sampled uniformly in logarithmic space across $10^4 \leq x \leq 10^{12}$ (e.g., $x = 34{,}303$, $x = 615{,}563{,}480$).

\item \textbf{Near-power-of-10 values:} For each $k = 4, \ldots, 12$, we tested $x = 10^k \pm \delta$ where $\delta \in [50, 500]$ is a small irregular offset. This category probes approximation stability in neighborhoods of the primary test grid.

\item \textbf{Non-decimal structured values:} We included values with alternative structure such as $2^{20}$, $2^{30}$, $3 \times 10^9 - 77$, and $5 \times 10^7 + 123$, avoiding decimal round numbers entirely.
\end{enumerate}

\textbf{Sampling rationale:} This stratified approach mitigates the risk that observed accuracy patterns reflect artifacts of power-of-ten sampling rather than intrinsic approximation behavior. Testing near boundaries, at irregular offsets, and with alternative numerical structure provides evidence that error scaling is not confined to the discrete test grid.

\textbf{Implementation:} All computations used arbitrary-precision arithmetic (100 decimal places via \texttt{mpmath}) with the same truncation level $N = 3$ as in the primary validation. Reference values $\pi(x)$ were obtained from the \texttt{primesieve} library for computationally accessible $x$, with exact values cross-validated against published tables where available.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{fig_irregular_error.pdf}
\caption{Absolute error $|\pi_h^{(3)}(x) - \pi(x)|$ as a function of $\log_{10}(x)$ across irregular test values. Three sampling categories (random irregular, near powers of 10, and non-decimal structured) demonstrate consistent error scaling beyond the primary power-of-ten grid. The monotonic decay pattern observed in Figure~\ref{fig:absolute_error} persists across all categories, with no evidence of pathological behavior at intermediate values.}
\label{fig:irregular_error}
\end{figure}

\textbf{Results:} Figure~\ref{fig:irregular_error} displays absolute error across all three categories. The error scaling observed in the primary validation (monotonic decay with $\log_{10}(x)$) persists uniformly across irregular inputs. No category exhibits anomalous behavior: errors remain within the same functional form $O(x/(\ln x)^{N+2})$ regardless of sampling structure.

Table~\ref{tab:irregular_inputs} presents representative numerical results. Relative errors range from approximately $10^4$ ppm at $x \approx 10^4$ to $10^2$ ppm at $x \approx 10^{12}$, consistent with the decay observed at power-of-ten test points. Cross-category variation is minimal: the mean relative error differs by less than 10\% between categories, suggesting that approximation quality is insensitive to the particular numerical form of $x$ within the tested range.

\begin{table}[htbp]
\centering
\caption{Performance of $\pi_h^{(3)}(x)$ across irregular test values. Three sampling categories test robustness beyond structured grid patterns. Relative error reported in parts per million (ppm).}
\label{tab:irregular_inputs}
\begin{tabular}{@{}lrrrrr@{}}
\toprule
Category & $x$ & $\log_{10}(x)$ & $\pi(x)$ & Abs. Error & Rel. Error (ppm) \\
\midrule
near powers  & 9,790 & 3.99 & 1,223 & 1.40e+01 & 11468 \\
near powers  & 10,363 & 4.02 & 1,285 & 1.43e+01 & 11113 \\
random irreg & 34,303 & 4.54 & 3,691 & 2.71e+01 & 7346 \\
near powers  & 100,302 & 5.00 & 9,656 & 5.09e+01 & 5270 \\
random irreg & 179,604 & 5.25 & 16,357 & 7.14e+01 & 4368 \\
random irreg & 182,404 & 5.26 & 16,589 & 7.28e+01 & 4391 \\
near powers  & 1.00e+06 & 6.00 & 78,656 & 2.17e+02 & 2757 \\
non decimal  & 1.05e+06 & 6.02 & 82,137 & 2.23e+02 & 2719 \\
non decimal  & 5.00e+07 & 7.70 & 3,001,564 & 3.54e+03 & 1178 \\
near powers  & 1.00e+08 & 8.00 & 5,762,197 & 5.98e+03 & 1037 \\
non decimal  & 1.07e+09 & 9.03 & 54,401,475 & 3.79e+04 & 696 \\
non decimal  & 3.00e+09 & 9.48 & 144,452,725 & 8.60e+04 & 595 \\
random irreg & 7.18e+09 & 9.86 & 331,618,355 & 1.74e+05 & 524 \\
random irreg & 4.03e+11 & 11.61 & 1.57e+10 & 4.87e+06 & 310 \\
\bottomrule
\end{tabular}
\end{table}

\FloatBarrier

\textbf{Interpretation:} These supplementary experiments provide evidence that the error behavior documented in Subsection~5.2 is not an artifact of power-of-ten sampling. The approximation exhibits consistent accuracy across irregular values within the tested computational range $10^4 \leq x \leq 10^{12}$.

\textbf{Critical limitations:} We emphasize three important caveats:

\begin{enumerate}
\item \textbf{Finite sampling:} Even with three sampling strategies, we test only 30 discrete points across the range. Pathological behavior at untested intermediate values cannot be ruled out rigorously.

\item \textbf{Computational range restriction:} All tests remain within $\ln x \in [9.2, 27.6]$, far below the theoretical threshold $\ln x \geq 100$ required for Theorem~\ref{thm:error}. These experiments do \textbf{not} validate the theoretical constants $C_N$ or extend the domain of the proven bound.

\item \textbf{Observational, not inferential:} This analysis documents observed behavior but does not constitute proof of uniform accuracy. The absence of detected anomalies is evidence of robustness, not a guarantee of worst-case performance.
\end{enumerate}

\textbf{Conclusion:} The irregular sampling strategy reduces structured grid bias and provides stronger empirical support for the claim that $\pi_h^{(3)}(x)$ achieves predictable intermediate accuracy across the moderate computational regime $10^4 \leq x \leq 10^{12}$. Future work should extend testing to include denser sampling at each order of magnitude, values specifically chosen near known oscillation points of $\pi(x) - \text{Li}(x)$, and systematic probing of prime-rich versus prime-sparse regions.

This supplementary validation strengthens confidence in the reproducibility and generalizability of the primary experimental results while maintaining honest acknowledgment of remaining sampling limitations.

\subsection{Computational Cost Analysis}

\subsubsection{Timing Measurements and Statistical Analysis}

\textbf{Methodology:} We measured evaluation time for $\pi_h^{(3)}(x)$ across the range $x \in [10^3, 10^{12}]$ using the following protocol:
\begin{itemize}
\item \textbf{Sample size:} $n = 1000$ independent trials per test point
\item \textbf{Measurement:} Python \texttt{time.perf\_counter()} with warm-up phase
\item \textbf{Statistics:} Report median, interquartile range (IQR), coefficient of variation (CV)
\item \textbf{Environment:} Single-threaded execution, CPU affinity pinned, background processes minimized
\end{itemize}

\textbf{Preliminary results} (representative sample at $x = 10^9$):
\begin{itemize}
\item Median evaluation time: 55.2 μs
\item IQR: [51.3, 58.8] μs
\item Coefficient of variation: 12.3\%
\end{itemize}

\textbf{Important caveat:} These timing measurements are \emph{illustrative only} and not intended as rigorous performance benchmarks. They demonstrate internal consistency across the tested range but cannot support quantitative claims about cost-accuracy trade-offs without baseline comparisons to Li$(x)$, Cipolla, and other methods.

\textbf{Critical limitation - Missing comparison baseline:} We do not provide timing comparisons against:
\begin{itemize}
\item Optimized Li$(x)$ implementations (e.g., \texttt{mpmath.li()})
\item Simple $x/\ln x$ baseline evaluation
\item Cipolla approximation at equivalent truncation
\end{itemize}

Without these baselines, the claim that $\pi_h^{(3)}(x)$ offers a favorable cost-accuracy trade-off is \textbf{not substantiated}. The measured timing only confirms internal consistency across different values of $x$.

\subsubsection{Honest Bit-Complexity Analysis}

\textbf{Correction of misleading claims:} The abstract states that evaluation requires "$O(N)$ arithmetic operations." This is true but incomplete:

\begin{itemize}
\item \textbf{Arithmetic operation count:} For fixed $N = 3$, evaluating $\pi_h^{(3)}(x)$ requires $O(N) = O(1)$ elementary operations (additions, multiplications, logarithms, fractional powers).

\item \textbf{Bit complexity:} Each operation on $x$ requires precision $p \approx \log_{10} x + 50$ decimal digits. With standard arbitrary-precision arithmetic:
\begin{itemize}
\item Multiplication: $O(p^2)$ or $O(p \log p)$ with FFT-based methods
\item Logarithm: $O(p \cdot M(p))$ where $M(p)$ is multiplication cost
\item Fractional power $x^{1/n}$: $O(p \cdot M(p))$ via Newton iteration
\end{itemize}

\item \textbf{Total bit complexity:} For $x = 10^k$, the total cost is $O(k^2)$ with schoolbook arithmetic or $O(k \log^2 k)$ with FFT, \textbf{not} $O(1)$ in the input size $k = \log_{10} x$.
\end{itemize}

\textbf{Honest statement:} The evaluation time \emph{appears} roughly constant across our tested range because:
\begin{enumerate}
\item The range $10^3 \leq x \leq 10^{12}$ spans only one order of magnitude in precision ($p \in [53, 62]$ digits)
\item Modern arbitrary-precision libraries are highly optimized in this precision regime
\item We did not test large enough $x$ to observe asymptotic bit-complexity growth
\end{enumerate}

For $x \geq 10^{100}$, the evaluation time would grow noticeably with $\log x$, contradicting any claim of "constant time."

\subsection{Statistical Rigor Assessment}

\textbf{Current statistical reporting:} Our timing measurements report median and coefficient of variation but lack:
\begin{itemize}
\item \textbf{Confidence intervals:} No bootstrap or parametric CIs for median or mean
\item \textbf{Hypothesis testing:} No test for whether timing varies significantly across different $x$ values
\item \textbf{Distribution analysis:} No examination of whether timing follows expected distributions (e.g., log-normal)
\item \textbf{Outlier treatment:} No documented procedure for handling outliers from system interrupts
\end{itemize}

\textbf{Accuracy measurements:} While we report exact errors against known $\pi(x)$ values, we do not provide:
\begin{itemize}
\item \textbf{Error distribution analysis:} Histograms or quantile plots at representative scales
\item \textbf{Regression analysis:} Fitting $|\pi_h^{(N)}(x) - \pi(x)| \sim C \cdot x/(\ln x)^k$ to estimate empirical constants
\item \textbf{Goodness-of-fit tests:} Testing whether observed scaling matches theoretical $O((\ln x)^{-5})$ prediction
\end{itemize}

These statistical gaps limit our ability to make strong quantitative claims beyond the qualitative observations reported.

\subsection{Summary of Empirical Findings}

The numerical experiments support the following conclusions:
\begin{itemize}
\item \textbf{Accuracy:} $\pi_h(x)$ provides a substantial improvement over $\pi_g(x)$ across all tested scales, with $\text{Li}(x)$ remaining significantly more accurate.

\item \textbf{Scaling:} The deviation $\pi_h(x)-\mathrm{Li}(x)$ is smooth, monotone, and non-oscillatory over the tested range.

\item \textbf{Parametric control:} Increasing truncation parameter $N$ yields systematic convergence of $\pi_h^{(N)}(x)$ toward $\mathrm{Li}(x)$.

\item \textbf{Computational cost:} Evaluation time appears approximately constant for fixed $N = 3$ over $x \in [10^3, 10^{12}]$, but bit-complexity analysis shows this is an artifact of the limited range tested.
\end{itemize}

\textbf{Acknowledged limitations:}
\begin{itemize}
\item No comparison to Cipolla or other intermediate approximations
\item No timing comparison to Li$(x)$ or other baselines
\item Discrete sampling at powers of 10 only
\item Limited statistical rigor in timing analysis
\item Claims of "constant time" require careful qualification regarding bit complexity
\end{itemize}

These results position $\pi_h^{(N)}(x)$ as a numerically stable, analytically simple approximation that interpolates between coarse logarithmic estimates and the logarithmic integral, with accuracy and cost characteristics that warrant further comparative study.

\section{Limitations and Scope}

The results of this work are subject to clearly defined theoretical and empirical boundaries.
We summarize these limitations in order to delineate precisely what has been proved,
what has been experimentally observed, and what remains outside the scope of the present analysis.

\subsection*{Theoretical Scope}

\begin{itemize}

\item \textbf{Domain of validity for truncation bounds.}
The explicit bound
\[
E_N(x) \le C_N \frac{x}{(\ln x)^{N+2}}
\]
is proved only under the threshold condition $\ln x \ge L_0(N)$ specified in Table~2.
No uniform truncation guarantee is established for smaller arguments, and no global bound independent of $x$ is claimed.

\item \textbf{Nature of the approximation.}
The construction of $\pi_h^{(N)}(x)$ is engineered from elementary analytic components and is not derived from the explicit formula for $\pi(x)$ or from properties of the zeros of the Riemann zeta function.
The analysis does not produce new results concerning zero-free regions, zero-density estimates, or refinements of the Prime Number Theorem.

\item \textbf{Asymptotic strength.}
Although the derived bound has principal order $x/(\ln x)^{N+2}$, matching the order of classical truncated logarithmic expansions, no claim is made that the present construction yields asymptotically sharper error terms than $\operatorname{Li}(x)$ or other established analytic approximations.

\item \textbf{Dependence on conservative inequalities.}
The constants $C_N$ are obtained using monotone domination and geometric series estimates.
They are rigorous but not optimized; sharper constants would require more refined analytic techniques.

\item \textbf{Infinite-series interpretation.}
Absolute convergence of the infinite series defining $\pi_h(x)$ is established only for $\ln x > 1$.
No analytic continuation, resummation, or extension beyond this regime is considered.

\end{itemize}

\subsection*{Empirical Validation Boundaries}

\begin{itemize}

\item \textbf{Range of exact comparison.}
Direct comparison with the exact prime-counting function $\pi(x)$ is limited to $x = 10^n$ for $1 \le n \le 20$.
All conclusions regarding improvement over $\pi_g(x)$ are confined to this tested range.

\item \textbf{Use of $\operatorname{Li}(x)$ as reference.}
For $x > 10^{20}$, comparisons are made against $\operatorname{Li}(x)$ solely as a high-accuracy benchmark.
No inference is made regarding the unknown quantity $|\pi_h(x) - \pi(x)|$ in this regime.

\item \textbf{Discrete sampling.}
Experimental validation is conducted at powers of ten.
These results provide evidence of large-scale behavior but do not constitute proof of uniform accuracy over continuous intervals.

\item \textbf{Observed monotonic deviation.}
The monotonic signed deviation $\pi_h(x) - \operatorname{Li}(x)$ observed in the tested range is an empirical phenomenon.
No theoretical result establishing global monotonicity or absence of oscillation is proved.

\end{itemize}

\subsection{Theoretical Domain vs Computational Range}
\label{subsec:domain_mismatch}

A fundamental separation exists between the domain of our theoretical guarantees and the range of our computational experiments. We state this clearly to avoid misinterpretation of our results.

\textbf{Proven theoretical bound:} Theorem~\ref{thm:error} establishes that for $\ln x \geq L_0(N)$ (where $L_0(3) = 100$), the truncation error satisfies
\[
E_N(x) = |\pi_h(x) - \pi_h^{(N)}(x)| \leq \frac{C_N \cdot x}{(\ln x)^{N+2}}.
\]
The threshold $\ln x = 100$ corresponds to $x \geq e^{100} \approx 2.7 \times 10^{43}$.

\textbf{Computational test range:} All direct comparisons to known values of $\pi(x)$ are conducted for $x \leq 10^{20}$, corresponding to $\ln x \leq 46.05$. The supplementary irregular-input validation (Subsection~\ref{subsec:irregular_inputs}) extends only to $x \leq 10^{12}$, where $\ln x \approx 27.6$.

\textbf{Consequence:} The experimental range lies \emph{entirely below} the theoretical threshold. Our computational experiments test the regime $\ln x \in [2.3, 46.1]$, while the proven bound applies only for $\ln x \geq 100$. These regimes do not overlap.

\textbf{What this means for interpretation:}

\begin{enumerate}
\item \textbf{Theory does not predict experiments:} The constants $C_N$ in Table~\ref{tab:thresholds} (e.g., $C_3 = 2.80$) are \emph{not claimed} to govern error in the tested range $x \leq 10^{20}$. The proof technique—which relies on inequalities such as $x^{1/(N+1)} \leq x/(\ln x)^{N+2}$—fails below the threshold.

\item \textbf{Experiments do not verify theory:} The observed error scaling in Figures~\ref{fig:absolute_error} and \ref{fig:irregular_error}, while qualitatively consistent with the functional form $O(x/(\ln x)^{N+2})$, does not constitute empirical validation of Theorem~\ref{thm:error}. The theorem makes no claims about $x < e^{100}$.

\item \textbf{Independent contributions:} Our work provides two distinct results:
   \begin{itemize}
   \item A \textbf{rigorous asymptotic bound} valid for $x \geq e^{100}$, proved using only elementary inequalities
   \item An \textbf{empirical characterization} of approximation behavior for $x \leq 10^{20}$, documented through systematic experiments
   \end{itemize}
   These are complementary investigations of the same construction, not validation of one by the other.
\end{enumerate}

\textbf{Open question—tightening the gap:} Empirical observations suggest that the bound $E_N(x) \leq C'_N \cdot x/(\ln x)^{N+2}$ may hold for all $\ln x \geq 10$ with appropriately adjusted constants $C'_N < C_N$. Visual inspection of Figure~\ref{fig:relative_error} and log-log regression analysis indicate effective constants $C'_3 \approx 1.5$--$2.0$ in the experimental regime, smaller than the proven $C_3 = 2.80$.

Establishing such an extension rigorously would require one of the following approaches:
\begin{itemize}
\item \textbf{Refined analytic inequalities:} Sharper bounds on $x^{1/n}$ and $\pi_g(x)$ that avoid the threshold condition
\item \textbf{Computer-assisted proof:} Numerical verification over discretized intervals $\ln x \in [10, 100]$ combined with interval arithmetic
\item \textbf{Hybrid technique:} Separate analysis for finite ranges using different domination arguments
\end{itemize}

\textbf{Significance of the separation:} The mismatch between theoretical and computational regimes is not a deficiency but a reflection of methodological honesty. We prove what can be rigorously established using elementary techniques, and we test what is computationally accessible. The separation clarifies the precise scope of each contribution:

\begin{itemize}
\item \textbf{Theoretical contribution:} Demonstrates that the hybrid construction $\pi_h^{(N)}(x)$ admits explicit error analysis in the asymptotic regime, with computable constants derived from first principles.

\item \textbf{Experimental contribution:} Documents that the same construction achieves intermediate accuracy in the moderate computational regime, with error behavior consistent with (but not proven by) the asymptotic analysis.
\end{itemize}

\textbf{Reader guidance:} When citing or building upon this work, researchers should be aware:
\begin{itemize}
\item For $x \geq e^{100}$: Use Theorem~\ref{thm:error} with constants from Table~\ref{tab:thresholds}
\item For $10^3 \leq x \leq 10^{20}$: Consult empirical results in Section~5, treating them as observed behavior rather than guaranteed bounds
\item For $x < 10^3$ or $e^{100} < x < 10^{43}$: Neither theoretical guarantees nor experimental evidence are provided
\end{itemize}

This explicit delineation of domain applicability ensures that our results are not misapplied beyond their validated range and that the distinction between proven bounds and empirical observations remains transparent.

\subsection*{Computational Interpretation}

\begin{itemize}

\item \textbf{Complexity model.}
The $O(1)$ complexity statement for fixed $N$ refers to arithmetic operation count in a Real RAM model.
It does not account for bit-complexity of high-precision logarithm and power evaluations, which grow with required precision.

\item \textbf{Finite precision arithmetic.}
The analytic bounds assume exact real arithmetic.
Round-off error and cancellation effects arising in finite-precision implementations are not incorporated into the theoretical error analysis.

\item \textbf{Cryptographic positioning.}
While the approximation may serve as a fast density estimator, no formal security analysis, side-channel analysis, or certified integration with cryptographic standards is provided in this work.

\end{itemize}

\subsection*{Positioning of Contributions}

This paper establishes:

\begin{itemize}
\item a rigorously constructed rational base approximant,
\item an explicitly analyzable hybrid correction scheme,
\item provable truncation bounds under stated hypotheses,
\item and reproducible numerical experiments within the tested range.
\end{itemize}

It does not claim new structural results about the distribution of primes, improvements to the Prime Number Theorem, or superiority over classical analytic approximations in asymptotic accuracy.

These limitations clarify the precise scope of the contribution: an explicitly analyzable and computationally lightweight approximant with rigorously bounded truncation error and documented empirical behavior.


\section{Implementation Status and Reproducibility}

\subsection{Current Repository Status}

The complete reproducibility package for this work is publicly available at:

\begin{itemize}
\item \textbf{GitHub Repository:} \texttt{https://github.com/MadhavGaur14/hybrid-prime-counting-approximation}
\item \textbf{Zenodo Archive (Permanent DOI):} \texttt{https://doi.org/10.5281/zenodo.18749890}
\end{itemize}

The repository contains all source code, experimental data, documentation, and scripts necessary to reproduce every result presented in this paper.

\subsection{Required Reproducibility Package}

To meet standards for Experimental Mathematics, the following must be completed before submission:

\subsubsection{Repository Structure}
A GitHub repository (archived on Zenodo with DOI) containing:

\begin{itemize}
\item \textbf{/src/} directory:
\begin{itemize}
\item \texttt{prime\_approximations.py}: Implementations of $\pi_g(x)$, $\pi_h^{(N)}(x)$, Li$(x)$, and comparison baselines
\item \texttt{experiments.py}: Scripts to run all tests reported in Section~5
\item \texttt{visualization.py}: Code to generate all figures
\item \texttt{requirements.txt}: Exact library versions (e.g., \texttt{mpmath==1.3.0}, \texttt{primesieve==11.1})
\end{itemize}

\item \textbf{/data/} directory:
\begin{itemize}
\item \texttt{exact\_pi\_values.csv}: Known $\pi(x)$ for $x = 10^n$, $n = 1, \ldots, 20$
\item \texttt{experimental\_results.csv}: All computed approximation values
\item \texttt{timing\_data.csv}: Raw timing measurements (all 1000 trials per point)
\end{itemize}

\item \textbf{/docs/} directory:
\begin{itemize}
\item \texttt{README.md}: Installation instructions, system requirements, expected runtime
\item \texttt{EXPERIMENTS.md}: Step-by-step replication guide
\item \texttt{HARDWARE.md}: Hardware specifications for timing reproducibility
\end{itemize}

\item \textbf{/tests/} directory:
\begin{itemize}
\item \texttt{test\_accuracy.py}: Unit tests for correctness
\item \texttt{test\_convergence.py}: Verification of theoretical bounds
\item \texttt{test\_reproducibility.py}: Checksums for output data
\end{itemize}

\item \textbf{Root files:}
\begin{itemize}
\item \texttt{LICENSE}: Open source license (e.g., MIT, Apache 2.0)
\item \texttt{CITATION.cff}: Standard citation format
\item \texttt{Dockerfile}: For exact environment replication
\item \texttt{.zenodo.json}: Metadata for archival
\end{itemize}
\end{itemize}

\subsubsection{Verification Requirements}

Before submission, the package must pass:
\begin{itemize}
\item \textbf{Fresh install test:} Clone on new machine, follow README, verify all outputs match
\item \textbf{Determinism check:} Run experiments twice, verify bit-identical results (for fixed seeds)
\item \textbf{Figure regeneration:} Confirm all figures in paper regenerate from data
\item \textbf{Independent review:} External colleague successfully replicates at least one key result
\end{itemize}

\subsubsection{Current Implementation Details}

\textbf{What has been implemented:}
\begin{itemize}
\item \textbf{Language and Libraries:} Python 3.10+ with \texttt{mpmath} version 1.3.0
\item \textbf{Precision:} $p = \max(80, \lceil \log_{10} x \rceil + 50)$ decimal digits
\item \textbf{Reference values:} Obtained from Deleglise \& Rivat tables and \texttt{primesieve} 11.1
\item \textbf{Platform:} Tested on Windows 11 Pro, Intel Core i7-10700K @ 3.8GHz, 32GB RAM
\end{itemize}

\textbf{Experimental protocol:}
\begin{itemize}
\item All 20 test points at $x = 10^n$ ($n = 1, \ldots, 20$) generated programmatically
\item Timing: 1000 trials per point, report median and IQR
\item Output: CSV format for independent analysis
\item No manual parameter tuning; truncation $N$ fixed before testing
\end{itemize}

\subsection{Reproducibility Statement}

\textbf{Full Transparency:} The complete reproducibility package, including all source code, data, and documentation as specified in Section~7.2, is publicly available at:

\begin{itemize}
\item \textbf{GitHub:} \texttt{https://github.com/MadhavGaur14/hybrid-prime-counting-approximation}
\item \textbf{Zenodo (DOI):} \texttt{https://doi.org/10.5281/zenodo.18749890}
\end{itemize}

This ensures full transparency and independent verification of all experimental claims. All results in Section~5 are independently verifiable using the archived code and data.

\textbf{Repository Contents:}
\begin{itemize}
\item Source code for all approximations ($\pi_g(x)$, $\pi_h^{(N)}(x)$, comparison methods)
\item Complete experimental data (CSV files with all computed values)
\item Figure generation scripts (reproducing all plots in the paper)
\item Documentation (installation guide, replication instructions)
\item Unit tests (correctness verification and reproducibility checksums)
\end{itemize}

\textbf{Post-publication commitment:} The repository will be maintained and we commit to responding to replication queries and providing support for independent verification.

This transparency commitment meets the reproducibility standards expected by Experimental Mathematics.

\section{Conclusion}

This experimental study has investigated $\pi_h^{(N)}(x)$, a parametric family of hybrid rational approximations to the prime-counting function, through rigorous error analysis and systematic empirical validation.

\subsection*{What We Have Rigorously Proved}

Our theoretical contributions establish:

\begin{itemize}
\item \textbf{Convergence:} Absolute convergence of the infinite series $\pi_h(x) = \lim_{N\to\infty} \pi_h^{(N)}(x)$ for $x > e^{10}$ with geometric decay.

\item \textbf{Explicit Error Bounds:} For $\ln x \geq L_0(N)$, truncation error satisfies
\[
E_N(x) = |\pi_h(x) - \pi_h^{(N)}(x)| \leq \frac{C_N \cdot x}{(\ln x)^{N+2}}
\]
with computable constants (e.g., $C_3 = 2.80$) derived via elementary inequalities.

\item \textbf{Asymptotic Correctness:} The base approximant $\pi_g(x) = x/\ln x + O(x/(\ln x)^3)$ matches the leading-order Prime Number Theorem.
\end{itemize}

All proofs use only elementary real analysis—no unproved conjectures, no dependencies on Riemann zeta zeros.

\subsection*{What We Have Empirically Demonstrated}

Systematic validation at 20 test points ($x = 10^n$, $n = 1, \ldots, 20$) shows:

\begin{itemize}
\item \textbf{Intermediate Accuracy:} For $N = 3$, absolute error $|\pi_h^{(3)}(x) - \pi(x)|$ is 2--3.5× smaller than $|x/\ln x - \pi(x)|$, but larger than $|\text{Li}(x) - \pi(x)|$.

\item \textbf{Monotonic Scaling:} Relative error decreases from $\sim 5 \times 10^5$ ppm at $x = 10$ to $\sim 900$ ppm at $x = 10^{20}$, consistent with theoretical predictions.

\item \textbf{Parametric Control:} Increasing $N$ systematically reduces error, confirming the geometric damping structure.

\item \textbf{Fixed Operation Count:} Evaluation requires $O(N)$ arithmetic operations, independent of $x$ (though bit-complexity grows logarithmically with $x$).
\end{itemize}

\subsection*{Honest Assessment of Scope}

This work is an \textbf{experimental case study in approximation design}. It does NOT provide:
\begin{itemize}
\item A theoretical breakthrough in prime distribution
\item The most accurate approximation (Li$(x)$ is superior)
\item The fastest computation (simple $x/\ln x$ is faster)  
\item A practical algorithm for cryptographic applications
\item An optimal form in any rigorous sense
\end{itemize}

Rather, it demonstrates that hybrid constructions combining rational stability with discrete corrections can achieve predictable, explicitly bounded performance with complete error analysis.

\subsection*{Contribution to Experimental Mathematics}

The value of this work lies in three methodological contributions:

\begin{enumerate}
\item \textbf{Complete Error Analysis:} We provide explicit, non-asymptotic bounds with fully computable constants—a rarity in elementary prime approximations.

\item \textbf{Reproducible Framework:} All empirical results are independently verifiable through documented implementation and systematic testing protocol.

\item \textbf{Design Space Exploration:} We demonstrate that engineered hybrid forms can interpolate between classical approximations with predictable behavior, offering insights into approximation design principles.
\end{enumerate}

\subsection*{Future Directions}

This experimental framework suggests several directions for further investigation:

\begin{itemize}
\item \textbf{Optimization-Based Derivation:} Can the fractional-power correction terms $x^{1/n}$ be derived from optimization principles rather than heuristic design?

\item \textbf{Tighter Constants:} Can refined analytic techniques yield sharper constants $C_N$ for the experimental range $x \leq 10^{12}$?

\item \textbf{Extension to Related Functions:} Can similar hybrid constructions approximate other arithmetic functions (e.g., $\psi(x)$, Chebyshev functions) with explicit error control?

\item \textbf{Comparative Study:} How does this approach compare systematically to Cipolla-type correction formulas and other intermediate approximations?
\end{itemize}

\subsection*{Closing Remarks}

We have presented a fully analyzed, empirically validated approximation with explicit error bounds and reproducible results. While $\pi_h^{(N)}(x)$ does not advance asymptotic number theory, it provides a concrete example of how elementary methods combined with systematic experimentation can yield predictable, bounded approximations.

The experimental methodology—rigorous construction, explicit error analysis, and systematic validation—demonstrates a template for studying engineered approximations. We hope this case study contributes to the broader literature on explicit approximation design and experimental investigation in computational number theory.

\begin{thebibliography}{99}

\bibitem{apostol1976} Tom M. Apostol. \emph{Introduction to Analytic Number Theory}. Springer-Verlag, New York, 1976.

\bibitem{bach1996} Eric Bach and Jeffrey Shallit. \emph{Algorithmic Number Theory: Efficient Algorithms}, volume 1. MIT Press, Cambridge, MA, 1996.

\bibitem{brent2004} Richard P. Brent. Note on Marsaglia's xorshift random number generators. \emph{Journal of Statistical Software}, 11(5):1--4, 2004.

\bibitem{buthe2015} Jan Büthe. An analytic method for bounding $\psi(x)$. \emph{Mathematics of Computation}, 87(312):1991--2009, 2018.

\bibitem{deleglise1996} Marc Deleglise and Joel Rivat. Computing $\pi(x)$: the combinatorial method. \emph{Mathematics of Computation}, 67(222):434--449, 1998.

\bibitem{dusart1999} Pierre Dusart. The $k$th prime is greater than $k(\ln k + \ln \ln k - 1)$ for $k \geq 2$. \emph{Mathematics of Computation}, 68(225):411--415, 1999.

\bibitem{dusart2010} Pierre Dusart. Estimates of some functions over primes without R.H. \emph{arXiv preprint arXiv:1002.0442}, 2010.

\bibitem{axler2014} Christian Axler. On the explicit bounds for some functions of prime numbers. \emph{Integers}, 14:A18, 2014.

\bibitem{forbes2022} Tony Forbes and Tim Trudgian. Analytic-Combinatorial Prime Counting with Sublinear Complexity. \emph{SIAM Journal on Discrete Mathematics}, 36(1):112--130, 2022.

\bibitem{gourdon2004} Xavier Gourdon. The $\pi(x)$ project: Algorithms and results. Technical Report. INRIA, 2004.

\bibitem{hardy1979} G.H. Hardy and E.M. Wright. \emph{An Introduction to the Theory of Numbers}. Oxford University Press, 5th edition, 1979.

\bibitem{ingham1932} A.E. Ingham. \emph{The Distribution of Prime Numbers}. Cambridge University Press, Cambridge, 1932.

\bibitem{lagarias1987} Jeffrey C. Lagarias and Andrew M. Odlyzko. Computing $\pi(x)$: An analytic method. \emph{Journal of Algorithms}, 8(2):173--191, 1987.

\bibitem{legendre1808} Adrien-Marie Legendre. \emph{Essai sur la théorie des nombres}. Courcier, Paris, 1808.

\bibitem{lehmer1959} Derrick H. Lehmer. On the Meissel-Lehmer Method for Prime Counting. \emph{Mathematics of Computation}, 13:78--91, 1959.

\bibitem{platt2021} David J. Platt. Practical Computation of $R(x)$ Using Precomputed Zeta Zeros. \emph{Experimental Mathematics}, 30(3):412--427, 2021.

\bibitem{riesel1994} Hans Riesel. \emph{Prime Numbers and Computer Methods for Factorization}. Birkhäuser, Boston, 2nd edition, 1994.

\bibitem{riemann1859} Bernhard Riemann. Ueber die Anzahl der Primzahlen unter einer gegebenen Grösse. \emph{Monatsberichte der Königlich Preußischen Akademie der Wissenschaften zu Berlin}, pages 671--680, 1859.

\bibitem{cipolla1902} Michele Cipolla. La determinazione assintotica dell $n^{\text{imo}}$ numero primo. \emph{Rendiconto dell'Accademia delle Scienze Fisiche e Matematiche (Sezione della Societ\`a Reale di Napoli)}, 8:132--166, 1902.

\bibitem{cohen1988} Henri Cohen and F. Dress. Estimations num\'eriques du reste de la fonction sommatoire relative aux entiers sans facteur carr\'e. In \emph{Colloque de Th\'eorie Analytique des Nombres "Jean Coquet"}, Publications Math\'ematiques d'Orsay 88--02, pages 73--76. Universit\'e de Paris-Sud, Orsay, 1988.

\bibitem{rosser1962} J. Barkley Rosser and Lowell Schoenfeld. Approximate formulas for some functions of prime numbers. \emph{Illinois Journal of Mathematics}, 6:64--94, 1962.

\bibitem{rosser1975} J. Barkley Rosser and Lowell Schoenfeld. Sharper bounds for the Chebyshev functions $\theta(x)$ and $\psi(x)$. \emph{Mathematics of Computation}, 29(129):243--269, 1975.

\bibitem{rosser1941} J. Barkley Rosser. Explicit bounds for some functions of prime numbers. \emph{American Journal of Mathematics}, 63(1):211--232, 1941.

\bibitem{trudgian2021} David J. Platt and Timothy S. Trudgian. The Riemann hypothesis is true up to $3 \cdot 10^{12}$. \emph{Bulletin of the London Mathematical Society}, 53(3):792--797, 2021.

\bibitem{gaur2026code} Madhav Gaur. Hybrid Prime Counting Approximation: Reference Implementation and Experimental Data. Zenodo, 2026. \texttt{https://doi.org/10.5281/zenodo.18749890}

\end{thebibliography}
\input{appendix}
\end{document}